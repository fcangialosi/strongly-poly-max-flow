\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{microtype}
\usepackage[shortlabels]{enumitem}
\usepackage{biblatex}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{float}
\usepackage{commath}
\usepackage{color, soul}
\usepackage{wrapfig}
\usepackage{xfrac}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={blue},
    citecolor={blue},
    urlcolor={blue}
}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes}
\usepackage{comment}

\let\comment\todo
\newcommand{\frank}[1]{\comment[nolist,color=blue!40]{f: #1}}
\newcommand{\david}[1]{\comment[nolist,color=orange!40]{d: #1}}
\newcommand{\katie}[1]{\comment[nolist,color=green!40]{k: #1}}

\addbibresource{main.bib}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{alg}[theorem]{Algorithm}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{definition}
\newtheorem{subroutine}{Subroutine}

\theoremstyle{definition}
\newtheorem{example}{Example}[section]

% Commonly used symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\fu}{f^{\mu}}
\newcommand{\nfi}{\nabla f_i}
\newcommand{\nfiu}{\nabla \fu_i}
\newcommand{\biu}{b_{i}^{\mu}}
\newcommand{\gij}{\gamma_{ij}}
\newcommand{\geu}{\gamma_e^{\mu}}
\newcommand{\giij}{\gamma_{ij}^{\mu}}
\newcommand{\vnott}{V \setminus t}
\newcommand{\din}{\delta^{\text{in}}}
\newcommand{\dout}{\delta^{\text{out}}}
\newcommand{\vsrc}{V^{s}}
\newcommand{\vsink}{V^{t}}
\newcommand{\vz}{V^{0}}
\newcommand{\fp}{(f,\mu)}
\newcommand{\fiju}{f_{ij}^{\mu}}

\newcommand{\tf}{\textsc{Tight-Flow}}
\newcommand{\ppn}{\textsc{Produce-Plentiful-Node}}
\newcommand{\filtration}{\textsc{Filtration}}
\newcommand{\es}{\textsc{Elementary-Step}}

\newcommand{\xands}{X \cap S}
\newcommand{\xnots}{X \setminus S}
\DeclareMathOperator{\Ex}{Ex}
\DeclareMathOperator{\Def}{Def}

\newcommand{\rewrite}[1]{\textcolor{red}{#1}}
\renewcommand{\todo}[1]{\hl{TODO: #1}}

\newcommand{\lpeq}[1] {
\begin{equation*}
\begin{aligned}
#1
\end{aligned}
\end{equation*}
}
\newcommand{\lpone}[3] {
& \underset{}{\text{#1}}
&& #2 \\
& \text{s.t.}
&& #3 
}
\newcommand{\lptwo}[4] {
\lpone{#1}{#2}{#3}\\
&&& #4
}
\newcommand{\lpthree}[5] {
\lptwo{#1}{#2}{#3}{#4}\\
&&& #5
}

\title{Strongly Polynomial Algorithms for Generalized Flow Maximization}
\author{Francis Cangialosi, Katie Lewis, David Palmer}
\date{}

\begin{document}
\maketitle
\section{Introduction}\label{sec:intro}
	\subsection{Problem Definition}\label{sec:problem}
	As in the traditional flow problem, given a
	graph $G = (V,E)$, the generalized maximum flow problem aims to maximize the
	total flow delivered to the sink node $t \in V$. Unlike the original flow
	problem however, each edge contains a gain factor $\gamma_e > 0$, which scales
	the flow passing through that edge. Intuitively, this scaling can be thought
	of in terms of real world examples such as exchange rates between currencies
	or physical transformations due to energy dissipation. The generalized maximum
	flow problem lacks several nice properties that are present in the traditional
	maximum flow problem, which makes it more challenging to solve. For example,
	the generalized problem is usually thought to be non-integral and,
	due to the gain factors, the total supply is not necessarily equal to the total
	demand. Additionally, the maximum flow--minimum cut theorem no longer applies
	since the flow along a path is not equal at every edge along that path.
    
	Until recently, the best known algorithms were all weakly polynomial. In
	2013, Végh developed the first strongly polynomial algorithm~\cite{Vegh2013}
	(i.e. one that depends only on the number of nodes and edges in the graph,
	and is independent of the size of their values). In 2017, Olver and Végh
	built on this work and developed an algorithm~\cite{Olver2017}
    that is faster by a factor of
	almost $O(n^2)$, resulting in a running time that is as fast as the best
	weakly polynomial algorithms even for small parameter values. These
	algorithms take advantage of the structural similarities between the
	generalized maximum flow problem and the minimum cost flow problem by
	adapting techniques from well-known combinatorial min cost flow algorithms,
    namely~\cite{Orlin1988}.
    
	\subsection{Structure and Contributions of the Paper}\label{sec:structure}
	In this paper, we aim to
	familiarize the reader with the recent algorithmic developments for the
	generalized flow maximization problem and provide intuition for the techniques
	used to achieve a strongly polynomial result. In Section~\ref{sec:prelim}, we more formally
	define the problem as a linear program and give an overview of the notation
	used in the rest of the paper. In Section~\ref{sec:motivate}, we develop the key techniques
	shared by both algorithms used to achieve a strongly polynomial bound for this
	problem. In Section~\ref{sec:2013}, we describe Végh's original $O(n^3m^2 \log n)$ strongly
	polynomial algorithm developed in 2014 and provide intuition. In Section~\ref{sec:2017}, we describe Olver and
	Végh's faster $O((m + n\log n)mn\log(n^2 / m))$ algorithm and highlight key
	aspects of the analysis that led way to the improved running time efficiency.
	Finally, in Section~\ref{sec:discussion}, 
    \rewrite{we conclude with open questions and present a few ideas
	for future work.}
    
\section{Preliminaries}\label{sec:prelim}

	\subsection{Network Notation}\label{sec:notation}
	%An instance of the generalized flow problem is specified as	
	Let $G=(V,E)$ be a directed graph with $n=\abs{V}$ and $m=\abs{E}$,
	$t \in V$ be a sink node and $\gamma \in \R_{>0}^E$ be the vector of flow gains
	across the edges. Although we have typically seen flow constraints defined in terms of
	edge capacities, recent work on the generalized problem defines flow
	constraints using node demands $b \in \R^{V \setminus t}$ instead (and leave
	edge capacities unbounded). Thus, an instance of the generalized flow problem
	is specified as $(G, t, \gamma, b)$.
	We adopt this formulation in our paper for
	convenience of analysis. As shown in~\cite{Vegh2013}, any problem defined with
	node demands can be easily transformed to an equivalent problem with
	capacities (and vice versa). 

	Although there is no explicit specification of a source node, the concept of
	node demands generalizes the concept of having multiple sources and multiple
	sinks: nodes with negative demand $-b$ act as sources because they can create
	up to $b$ units of flow, and nodes with positive demand $b$ act as sinks 
	because they consume at least $b$ units of flow. Thus, (excluding $t$), we denote the subset of
	nodes with negative demand as $\vsrc$, the subset of nodes with positive
	demand as $\vsink$, and the remaining nodes with zero demand as $\vz$.

	For a subset of the nodes $S \subseteq V$, we define $\din(S)$ and
	$\dout(S)$ to be the set of incoming and outgoing edges, respectively,
	and let $d_i = |\din(\{i\}) \cup \dout(\{i\})|$ be the total degree of a
	node $i \in V$.

	Then we can define the net flow at a node $i$ as the amount of flow that
	reaches a node (scaled by the gain factor on the incoming edges) minus the
	amount of flow that leaves the node: 
	$$ \nfi \coloneqq \sum_{e \in \din(i)} \gamma_e f_e - \sum_{e \in \dout(i)} f_e.$$

	The residual graph $G_f = (V,E_f)$ is defined as in the traditional problem,
	except the reverse residual edges $(j,i) \in E_f$ have inverted gain 
	$\gamma_{ji} = 1 / \gamma_{ij}$ and negated flow $f_{ji} =
	-\gamma_{ij}f_{ij}$. If the cumulative gain of a cycle $C$ ($\prod_{e \in C} \gamma_e$)
	in $G_f$ is greater than 1, we call $C$ a ``flow-generating cycle.'' 
	This captures the idea of arbitrage: sending a unit of flow around the cycle
	generates a net surplus.\frank{We need to mention that we assume no flow generating cycles, and how to cancel them}





	\subsection{Linear Program Formulation}
	\label{sec:lp}

	We formulate the generalized flow maximization problem with demands as in the 2017
	paper~\cite{Olver2017}:
	\vspace{-0.35cm}
%		\begin{align*}
%		\text{max} \quad
%		\nabla f_t& \\
%		\text{s.t.} \quad \tag{P}
%		\nabla f_i &\geq b_i \quad \forall i \in V \setminus t \\
%		f &\geq 0
%		\end{align*}        
%
%		\noindent The dual of (P) is shown below on the left. If we set
%		\rewrite{$\mu = \mu$}, we arrive at the formulation on the right,
%		as given in~\cite{Olver2017}:
%

    \begin{align*}\tag{P}
    \max \quad \nabla f_t& \\
    \text{s.t.} \quad
    \nabla f_i &\geq b_i \quad \forall i \in V \setminus t \\
    f &\geq 0
    \end{align*}

	The dual linear program, given on the left below, has a variable $\eta_i$ for
	every node $i$. 
	Making the change of variables $\eta_i = - \mu_t / \mu_i$ yields the
	dual optimization problem on the right.
    \vspace{0.5cm}

	\begin{tabular}{rcll}
		\hspace*{-1.05cm}
	\resizebox{0.37\textwidth}{!}{
		\fbox{
	\begin{minipage}{0.35\textwidth}
	\begin{alignat*}{4}
    \min &\quad &\sum_{i \in V \setminus t} b_i \eta_i \\
    \text{s.t.}
    &   &\gij \eta_j - \eta_i &\geq 0 \quad &&\forall\; &&(i, j) \in E \\
    &   &                     &             &&          &&\ i, j \neq t \\
    &   &\gamma_{ti} \eta_i &\geq -1 \quad  &&\forall\; &&(i, t) \in E \\
    &   &-\eta_i &\geq \gamma_{it} \quad    &&\forall\; &&(t, i) \in E \\
    &   &\eta_i &\leq 0 \quad               &&\forall\; &&i \in V \setminus t
    \end{alignat*}
	\end{minipage}
}
} & 
	$\xrightarrow{\hspace*{0.25cm}\eta_i = - \mu_t / \mu_i\hspace*{0.25cm}}$
	&
	\resizebox{0.37\textwidth}{!}{
		\fbox{
	\begin{minipage}{0.4\textwidth}
    \begin{alignat*}{3}
    \max &\quad &\mu_t \sum_{j \in V \setminus t} \frac{b_j}{\mu_j}  \\
    \text{s.t.}
    &   &\gij \mu_i &\leq \mu_j \quad &&\forall\; (i, j) \in E \\
    &   &\mu_i &\in \R_{>0} \cup \infty \quad &&\forall\; i \in V \setminus t \\
    &   &\mu_t &\in \R_{>0}
    \end{alignat*}
	\end{minipage}
}
} & \hspace*{0.4cm}(D)
\end{tabular}
\vspace{0.5cm}

        
	A dual solution $\mu$ is known as a \emph{labeling}, and we can speak of feasible and
	optimal labelings. \frank{@david: do you have an intuitive sense of \textit{why} we take the approach of solving the dual? Is it because it simplifies the problem as we say below or is there more to it?} 
    We make the simplifying assumption that the sink is
	reachable from every node, which ensures $\mu$ will be finite. (\cite{Olver2017} shows a simple transformation that allows us to guarantee this assumption.) 
	Complementary slackness requires that for an optimal flow $f^*$ and optimal labeling $\mu^*$,
    \begin{equation}\label{eqn.cs} \tag{CS}
    f^*_{ij} > 0 \implies \gamma_{ij}\mu^*_i = \mu^*_j.
    \end{equation}
	This suggests a particular interpretation of the dual variables. Imagine that
	the nodes represent currencies and the gains represent exchange rates between those currencies.
	A maximum generalized flow is the currency trading strategy that maximizes the value in currency
	$t$ at the end. We can interpret a label $\mu_i$ as the value of one dollar in currency $i$.
	The dual constraint $\gamma_{ij} \mu_i \leq \mu_j$ means that under the labeling $\mu$,
	it is not possible to create value by converting currencies.
	The complementary slackness condition means that the currency trading strategy
	only makes trades that exactly preserve value.
	
    % $\mu$ is a value assignment. We're rewriting the problem in a different
		% currency. The relabeled dual problem is equivalent to the original dual
		% problem, the constraints for tight edges become replace $\mu$ with $\mu'$
		% in all the constraints. There's a dual problem which solves for $\mu'$ on
		% each node, and you can then get a solution to the original problem which is $\mu*\mu'$

	Given a feasible dual solution $\mu$, we can apply the following \textbf{relabeling}
    transformation to obtain a new problem instance with gains $\gamma^\mu$ and demands $b^\mu$
    and a primal solution to the new instance:
	\[ \gamma_{ij}^\mu \coloneqq \gamma_{ij} \frac{\mu_i}{\mu_j} \quad
	b_i^\mu \coloneqq \frac{b_i}{\mu_i} \quad
    f_{ij}^\mu \coloneqq \frac{f_{ij}}{\mu_i} \quad
	\nabla f_i^\mu \coloneqq \frac{\nabla f_i }{\mu_i} \]
    The relabeled dual problem is equivalent to the original dual problem. In particular,
    \begin{claim}
    $f^\mu$ is a feasible (resp. optimal) solution to the relabeled primal problem with gains
    $\gamma^\mu$ and demands $b^\mu$ if and only if $f$ is a feasible (resp. optimal) solution
    to the original problem with gains $\gamma$ and demands $b$. \katie{which original problem?}
    \end{claim}
    \begin{claim}
    If $\mu$ is a feasible solution to the original dual problem and
    $\bar{\mu}^*$ is an optimal solution to the relabeled dual problem with gains
    $\gamma^\mu$ and demands $b^\mu$,
    then the vertex-wise product $\mu\bar{\mu}^*$ is an optimal solution to the original dual problem.
    \end{claim}
    We will often use relabeling to simplify the problem while preserving its structure.\frank{I think this sentence is important, but the above claims are a bit confusing. Can we be slightly less formal and summarize the claims in one or two sentences?}
    
    \begin{definition} \label{def.tight}
	An edge $e$ is known as \textbf{tight} with respect to $\mu$ if $\gamma_e^\mu =1$.
	For an optimal pair $(f, \mu)$, complementary slackness is exactly the condition that
	flow only flows along tight edges. We denote the set of tight edges under
	$\mu$ by
    \[ \tau(\mu) = \{e \mid \geu = 1\}. \]
    The \textbf{tight graph} consists of these edges and their endpoints.
	\end{definition}
    
    \begin{definition} The \textbf{excess} of a flow at a node $i$ is the net flow left
    there after subtracting demand, $\nfi - b_i$. When this quantity is negative, it is
    described as \textbf{deficit}. Given $f$ and $\mu$, we can define relabeled excess
    at a node similarly as $\nfiu - \biu$.
    
    Given flow values and labels, the \textbf{total excess and deficit}
    of the flow with respect to the labels is defined as follows:
	\begin{align*}
	\Ex(f,\mu)  &\coloneqq \sum_{i \in V \setminus t} \max \{ \nfiu - \biu, 0 \} &
	\Def(f,\mu) &\coloneqq \sum_{i \in V \setminus t} \max \{ \biu - \nfiu, 0 \}
	\end{align*}
    \end{definition}
  	\frank{add something about individual node excess}
  
  
\section{Motivating Ideas and Techniques}
\label{sec:motivate}
	\subsection{Solving the Dual Problem Combinatorially}
	\label{sec:motivate-dual}
    	%* Talk about idea of solving dual 
      %  	* Key insight about this algorithm is that they maintain $f^{\mu}$ (without any extra work apparently) to be integral
      %      * Allows normal flow algorithms 
      %      * Is maintaining solution that is "close enough to feasible primal" with complementary slackness unique to this algorithm?
      %      * Draw parallels between min cost flow and generalized max flow (give more clear and intuitive explanation than the Truemper paper)
	
	A weakly polynomial algorithm is easily achievable by applying ellipsoid or
	interior point methods to the linear program (P). To achieve a strongly polynomial
	bound, the linear program must be reduced to a more combinatorial problem.
	
	Both algorithms discussed below employ a simple (and strongly polynomial)
	reduction to the dual problem (D).
    Given optimal dual labels $\mu$, consider the primal problem relabeled by $\mu$.
    Complementary slackness dictates that for any optimal flow $f$,
    $\giij = \gij\mu_i/\mu_j = 1$ whenever $f_{ij} > 0$. As such, the conservation
    constraint for the relabeled optimum flow $f^\mu$ takes the form
    \[ b^{\mu} \leq \nabla f^{\mu}
    = \sum_{(j,i) \in \din(i)} \gamma_{ji}^{\mu} \fu_{ji} - \sum_{(i,j) \in \dout(i)} \fu_{ij}
    = \sum_{(j,i) \in \din(i)} \fu_{ji} - \sum_{(i,j) \in \dout(i)} \fu_{ij}. \]
    This is the conservation constraint of an ordinary maximum flow problem with demands
    $b^\mu$, and indeed one can compute the optimal $f^\mu$ as an
    ordinary maximum flow on the tight graph---a strongly polynomial operation.
    Then it is easy to recover $f$ from $f^\mu$.
    
    In fact, modulo an ordinary maximum flow computation, the dual problem reduces to computing the set of tight edges.
		\frank{@david: what is THE set of tight edges? I think tight is a little overloaded, because really we mean tight under a particular $\mu$ right?}
    Since regular maximum
    flow has already been solved in strongly polynomial time, it suffices to find a strongly polynomial
    algorithm to compute a set of tight edges consistent with a dual optimal solution. This problem
    is inherently combinatorial, suggesting that it can be solved in strongly polynomial time.
    
    \subsection{Contracting edges}\label{sec:contract}
    To organize the search for tight edges, both algorithms inductively reduce the graph.
    Suppose an edge $(i, j)$ is known to be tight for any dual-optimal solution $\mu$. This
    imposes a constraint $\giij = 1$ for all such $\mu$, or, i.e., $\gij \mu_i = \mu_j$.
    Imposing a constraint of this form reduces the dimension of the solution
    space by one and is equivalent to removing one variable from optimization. Indeed,
    suppose we solve the equivalent problem instance $(\biu, \giij)$. Then the new constraint
    becomes $\mu_i' = \mu_j'$. In order to avoid working with extra constraints, we might
    as well remove one of the variables entirely by collapsing the edge $(i, j)$, leaving a single
    node $j$ with a single dual variable $\mu_j$. It remains to set the demand $b_j$ so
    that the problem remains equivalent. Rewritten in terms of the variables in
    the modified problem, the relevant terms of the dual objective are
    \[ \mu_t' \left(\frac{b_i^\mu}{\mu_i'} + \frac{b_j^\mu}{\mu_j'}\right)
     = \mu_t' \left(\frac{b_i^\mu + b_j^\mu}{\mu_j'}\right). \]
	So the operation of collapsing $i$ into $j$ along $(i, j)$ and summing their demands
	preserves the objective and constraints.
	
	We can make one additional simplification: if the contraction leads to parallel edges,
	only the one with the highest gain is maintained. This is motivated by the primal problem: in
	the absence of edge capacity constraints, there is no reason to send flow along an edge of
	lower gain when one of higher gain is available. Thus, culling parallel edges preserves the
	essential features of the problem.
	
	\subsection{Progress} \label{sec:progress}

	%Following from the previous two sections, in order to optimize the dual,
	%both algorithms make progress by alternating between finding contractible
	%edges and contracting them.
	The heart of both strongly polynomial algorithms is a procedure for
	identifying contractible edges and then contracting them. The key
	differences between the two algorithms are in 
	\textit{how} they find contractible edges, which
	we will describe in Sections~\ref{sec:2013} and~\ref{sec:2017}. These differences
	allow Olver and Végh \cite{Olver2017} to achieve a better running time bound than
	Végh \cite{Vegh2013}.

	Contractions continue until one of the following cases occurs:
	\begin{enumerate}[(i),itemsep=0mm]
	\item the contracted graph has just one node remaining, i.e., every node has been merged
	into the sink $t$. In this case, the set of previously identified
	contractible edges forms a spanning tree. Given a value for $\mu_t$, the tightness
	constraints then uniquely determine the values of the optimal $\mu$ at all other vertices.
	
	\item the contracted graph has multiple nodes, but the (merged)
	demands $\biu$ are all zero. In this case, the dual objective on the contracted
	graph is identically zero, and the primal solution $f \equiv 0$ is feasible,
	with a primal objective value of zero. By strong duality, any feasible dual
	solution on the contracted graph is optimal---in particular, the feasible
	solution produced during the contractions is optimal.
	
	In this case, the set of previously identified contractible edges
	forms a spanning forest where each component is identified with one node
	of the contracted graph. Given the values of the dual solution at these vertices,
	the tight spanning forest provides a recipe for reconstructing
	an optimal dual over the original graph.
	
	From the primal perspective, each component of the tight forest has net ($\mu$-relabeled)
	demand zero. As such, the flows in separate components are independent, and the objective
	value only depends on the flow on the tight tree component containing the sink $t$.
	As observed in Section \ref{sec:motivate-dual}, this
	can be computed (in strongly polynomial time)
	by an ordinary maximum flow computation over the edges of the tight tree containing $t$.
	\end{enumerate}
    
	\begin{example}
	Consider the simple flow network in
	Figure~\ref{fig:ex-spanning}. By inspection, we can see that the maximum flow $f^{*}$
	should send 8 units of flow from $i$ to $t$ and 8 units of flow from $j$ to $t$,
	resulting in 6 units at the sink. On the right, we show the three possible spanning
	trees of tight edges for this graph.
	For each tree, starting with $\mu_t = 1$, we can compute $\mu_i$
	and $\mu_j$. In (a), the dual is not feasible because
	$\gamma_{it}\mu_i = 4 \nleq 1 = \mu_t$.
	In (b) and (c) all of the dual constraints are feasible,
	but (b) produces a greater objective value
	than (c). Solving a max flow on just the tight edges of (b) yields the max value
	we expect, 6. The optimal collection of tight edges encodes all the information necessary
	to reconstruct the optimal solution.\frank{todo: mention u* is negative 6 because of change of variables}
	\begin{figure*}[h]
	\centering
	\begin{minipage}{.49\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figs/tight}
	\end{minipage}
	\begin{minipage}{.49\textwidth}
		\centering
		\includegraphics[width=.75\linewidth]{figs/tight-span-2}
	\end{minipage}
	\label{fig:ex-spanning}
	\caption{(left) Example flow network. (right) All three possible tight
	spanning trees and their respective dual solutions.
	Dark edges are tight ($\gamma^{\mu} = 1$), gray dashed edges are not.}
	\end{figure*}
	\end{example}

	\subsection{Contractibility Certificates} \label{sec:cert-contract}
	Complementary slackness tells us that if there is a primal optimal flow
	$f$ with positive value
	on some edge $e$, then $e$ is tight with respect to \emph{every} dual
	optimal solution---that
	is, $e$ is contractible. So a primal optimal flow provides a certificate
	for contractibility.
	It would seem this is of no use to us, as computing a primal optimal flow is the entire
	problem we are trying to solve. However,
	an optimal flow is a vastly superfluous certificate.
	
	Suppose instead that we have any flow $g$ and a bound on the difference
	between $g$ and an optimal
	flow $g^*$. Then if $g$ had a sufficiently large value on some edge $e$, we would know that
	$g^*_e > 0$. This idea is formalized in the following lemma, adapted from Lemma 3.3 of
	\cite{Olver2017}.
	
  	\begin{definition} \label{def:conservative}
  	Let $f$ be a feasible solution to (P).
	$\mu$ is a \textbf{conservative labeling} for $f$ if $\mu$ is
	a feasible solution to (D) and all edges with positive flow are tight. In this case,
	$(f, \mu)$ is called a \textbf{conservative pair}.
	\end{definition}

	\begin{lemma} \label{lem.bound-dist}
	Suppose $(g, \mu)$ is a conservative pair. Then there is some optimal flow $g^*$
    uniformly ``close'' to $g$:
    \[ \|(g^*)^\mu - g^\mu\|_\infty \leq \Ex(g, \mu). \]
    \end{lemma}
    \begin{proof}
    The intuition is that the optimal flow improves on the feasible flow by
    sending more of its excess to the sink. Thus, the difference $g^* - g$
    is a flow of value at most $\Ex(g, \mu)$ at $t$. The maximum edge value of
    the flow is bounded, in turn, by its total flow value.
    
	Since we are working with generalized flows, it will take a little work to formalize
	this idea. First, among all the optimal solutions to (P), let $g^*$ be the one
	which minimizes the total ($L^1$) difference $\|g^* - g\|_1$. Let $\mu^*$ be an
	optimal solution to (D). Then $(g^*, \mu^*)$ are a conservative pair---this is precisely
	the complementary slackness condition they satisfy.
	
	Let $h = g^* - g$. That is,
	\[ h_{ij} \coloneqq \begin{cases}
							g^*_{ij} - g_{ij} & \text{if } (i, j) \in E, \,
														   g^*_{ij} > g_{ij} \\
							\gij(g_{ij} - g^*_{ij}) & \text{if } (j, i) \in E, \,
																 g_{ji} > g^*_{ji}
						\end{cases} \]
    Observe that if $h_{ij} > 0$, then either $(i, j) \in E$ and $g^*_{ij} > 0$ or
    $(j, i) \in E$ and $g_{ji} > 0$. In other words,
    $(i, j) \in E_g$ and $(j, i) \in E_{g^*}$.
    
    \begin{claim} \label{claim:nocycles} $h$ is acyclic. \end{claim}
    \begin{proof}[Proof of Claim]
    Suppose that $h$ contains a cycle $C$. Then consider the net gain around $C$:
    \[ 1 \geq \gamma^\mu(C) = \gamma(C) = \gamma^{\mu^*}(C) \geq 1, \]
    as $\gamma^\mu \leq 1$ on $E_g$ and $\gamma^{\mu^*} \leq 1$ on $E_{g^*}$.
    As $\gamma(C) = 1$, we can slightly modify $g^*$ over the whole cycle to
    decrease $\|h\|_1$, contradicting the assumption on $g^*$.
    \end{proof}
    
    Now $h$ may be decomposed into a set of paths. Because $h$ is supported in $E_g$ and
    $\gamma^\mu \leq 1$ on $E_g$, flow can only decrease along these paths.
    As such, the total flow along any edge is at most the total flow leaving nodes:
    \[ \|h^\mu\|_\infty \leq \sum_i \max\,\{0, -\nabla h_i^\mu\}
     = \sum_i \max\,\{0, \nabla g_i^\mu - \nabla (g^*)_i^\mu\}
     \leq \sum_i \max\,\{0, \nabla g_i^\mu - \biu \}, \]
     which is $\Ex(g, \mu)$ as desired.
    \end{proof}
    The lemma suggests a concrete certificate for contractibility. An edge $e$ is
    \textbf{abundant} with respect to a conservative pair $(f, \mu)$
    if $f^\mu_e > \Ex(f, \mu)$.
    Lemma \ref{lem.bound-dist} shows that abundant edges are contractible.
    
    The contractibility certificate developed above determines the broad outlines
    for any contraction-based generalized flow algorithm. The algorithm's objective
    is to increase the maximum edge flow while keeping the total excess bounded.
    In order to achieve this, it will have to increase the demand values. Thus, the algorithm
    will consist of alternating \emph{scaling} and \emph{augmentation} phases.
    In the scaling phases, the dual variables $\mu$ are adjusted so that
    the values $|\biu|$ will increase. In the complementary augmentation phases,
    the algorithm spreads excesses around to keep them small.
    
    Since the dual scaling will be a node-wise operation, it is more convenient
    to use a node-wise certificate of contractibility than the edge-wise one described
    above.
    

\section{The Initial Strongly Polynomial Algorithm}
\label{sec:2013}
	%\subsection{Introduced Concepts and Notation}
    \label{sec:2013-notation}

%Notes for "Intuitive Section 4":

%-continuous scaling -> find contractible edges -> optimal dual -> optimal primal
%-continuous scaling -> breaks down usual $\Delta$ phases so won't over-shoot labeling
%-maintain feasible primal solution by keeping $\Delta$-feasible pair -- ensures security reserve $e_i >= R_i$
%-maintain bound on excess for individual nodes
%-trying to minimize max excess -- why?
%-push flow/excess around to create/identify abundant arc $(f_ij) >= 17m\Delta$
%-nodes with relabeled b magnitude $>= 20mn\Delta$ must have abundant arc adjacent
%-Pushes excess around by augmenting and relabeling; if algorithm gets "stuck", uses filtration to push flow around and even out excess in an "isolated" part of graph (not part of frontier)
%-algorithm maintains that b/delta is non-decreasing
%-cycle canceling only once in beginning -- explain why and assumptions?


A high-level overview of Végh's initial algorithm \cite{Vegh2013}
is shown below. This algorithm is characterized by a scaling factor $\Delta$,
which controls the amount of flow that can be augmented at each iteration and the threshold for
determining which edges are contractible. Sections \ref{sec:findconserv2013} and
\ref{sec:findcontr2013} expand on the details and provide intuition for steps
(1) through (4) below.

\begin{enumerate}[(1),itemsep=0mm]
\item Find an initial feasible flow $\bar{f}$ and set the initial $\Delta$ to
	  $\max_{i \in V\setminus T} \nfiu - \biu$
\item Run a cycle-canceling algorithm to get a conservative labeling, $\mu$
\item Find contractible edges and contract until a stopping condition is met
	(\emph{cf.} Section \ref{sec:progress} Case i) in one of the following ways:
	\begin{enumerate}
		\item augment $\Delta$ units of flow
		\item increase (resp. decrease) $\mu$ (resp. $\Delta$) by a constant factor
		\item send flow from isolated parts of the graph directly to the sink
	\end{enumerate}
\item Expand and find optimal primal solution via max flow (\ref{sec:motivate-dual}) \todo{check if correct ref and need to add section to talk about this or already covered in 3?}
\end{enumerate}

%\begin{definition}
%	For $\Delta \geq 0$, the \textbf{$\Delta$-fat graph} consists of all
%	forward edges $(i, j)$ and those reverse edges $(j, i)$ with $f_{ij}^\mu > \Delta$.
 %   \end{definition}

\subsection{Finding and Maintaining Conservative Pairs}
\label{sec:findconserv2013}
Following from primal-dual slackness, the optimality conditions for the primal are (i) $e_i = 0 \quad \forall i \in V-t$ and (ii) $\giij = 1$ if $f_{ij} > 0 \quad \forall ij \in E$. The first condition is satisfied at the end of this algorithm when a max flow computation transforms the optimal dual solution into an optimal primal solution \katie{explain better} (proof in \cite{article}). The second condition is achieved by finding and maintaining conservative pairs (Definition~\ref{def:conservative}) throughout the algorithm.

To find an initial conservative pair (f,$\mu$), a feasible flow $\bar{f}$ with no flow-generating cycles must first be found. $\bar{f} \equiv 0$ is a trivial feasible solution to the capacitated version of the primal; this can be used to find an initial feasible solution for the uncapacitated representation by following the transformation in the paper \cite{article} (this just requires adding a few additional edges and nodes). Flow-generating cycles are then removed by the \textsc{maximum-mean-gain cycle-canceling} algorithm, introduced by Goldberg et. al \cite{Goldberg:1991:CAG:105014.105022}. This is required because there cannot be flow-generating cycles if there is a finite conservative labeling $\mu$ as shown in Claim~\ref{claim:nocycles}. The initial conservative labeling is then determined by defining $\mu_t \coloneqq 1$ and then defining $\mu_i \coloneqq$(max $\gamma$ along path from i to t)$^{-1} \quad \forall i \in V-t$. Finally, a max flow procedure is run to find a generalized flow f with respect to $\mu$, which gives us a conservative pair (f,$\mu$). At the end of this initialization phase, we also set the starting value of $\Delta$ equal to $\max_{i \in V\setminus T} \nfiu - \biu$, which is the maximum relabeled excess for any node (excluding t). This value is chosen because $\Delta$ controls the amount of flow augmented at each iteration, and it wouldn't be feasible to send more flow along a path than the maximum excess. Since $\Delta$ is monotonically decreasing in the algorithm, we initialize it to the maximum feasible value. 

Once a conservative pair $(f, \mu)$ is found, the algorithm maintains a relaxed version of the pair called a $\Delta$-conservative pair. This key new idea allows a bounded amount of flow to pass along non-tight edges, while maintaining a ``security reserve''. The security reserve requires that the excess of a node i must be $\geq$ the incoming flow on non-tight edges; this keeps a conservative pair within reach because setting the flow on all non-tight edges to zero still leaves non-zero excess on every node and results in a feasible flow with no flow-generating cycles. Additionally, $\Delta$-conservative pairs require that the flow on a non-tight edge ij must be $\leq \Delta$. As $\Delta$ decreases, the amount of flow on these non-tight edges decreases and the amount relabeled excess that is created when the non-tight edges are reset to zero also decreases. This means the algorithm gets closer to an optimal solution as $\Delta$ decreases.

\subsection{Identifying Contractible Edges}
\label{sec:findcontr2013}
The measure of progress in this algorithm is finding a contractible edge in a
strongly polynomial number of steps. An edge is contractible when the flow on
that edge is very large compared to $\Delta$; however, rather than keeping track
of this ratio, the algorithm maintains a non-decreasing ratio
$\frac{\abs{b_i^\mu}}{\Delta}$. The original paper uses proof by contradiction
to show that if a node $i$ has $\abs{b_i^\mu} \geq 20mn\Delta$, then it must
have a contractible edge adjacent to it.\frank{We might want to at least sketch
the proof for why this is true}

While finding and contracting edges, the algorithm maintains several sets as defined below:
	\begin{align*}
	L&\text{: set containing sink and ``low relabeled excess'' nodes with excess of node } i < (d_i +1)\Delta \\
	H&\text{: set of ``high relabeled excess'' nodes with  excess of node }i \geq (d_i + 2)\Delta \\
	R&\text{: set of nodes reachable on a tight path from $T_0$ } \\ 
	D&\text{: set of nodes $i \in V \setminus t$ with } \abs{b_i^\mu} \geq \Delta / n
	\end{align*}
	\frank{Why is $D$ constructed in this way?}
\textbf{\textsc{Augmentation}} As shown in step 3a of the algorithmic overview above, each iteration of the main while loop (the ``finding and contracting edge''
while loop) first looks to augment $\Delta$ units of flow on
tight paths from high relabeled excess nodes in set H to low relabeled
excess nodes (including the sink) in set L. The algorithm only augments on tight
paths because these paths are the highest-gain paths since $\gamma_{ij}^\mu = 1$ (recall the dual constrains $\gamma_{ij}^\mu$ to be $\leq 1$)
If the algorithm is not able to augment, then there must either be (i) no
reachable low excess nodes to augment to or (ii) no high excess nodes to push
flow from. \\ \\
\textbf{\textsc{Scaling}} The subroutine $\es$ resolves both potential problems by increasing $\mu$ and
decreasing $\Delta$ by a factor of $\alpha > 1$ (step 3b above)\frank{How is this value chosen?}. Increasing $\mu$ for the
non-reachable nodes (i.e. nodes in set V $\setminus$ R) causes the ratio
$\gamma_{ij}^\mu \frac{\mu_i}{\mu_j} < 1$ 
to increase for edges from nodes in $R$
to nodes in $V \setminus R$.
\frank{It might be helpful to refer to this as edges crossing the V-R cut}
This can cause an edge between a reachable and a
non-reachable node to become tight (i.e. $\gamma_{ij}^\mu \frac{\mu_i}{\mu_j} =
1$), which may add a low excess node to the set of reachable nodes. On the other
hand, decreasing $\Delta$ decreases the lower threshold for $H$, which might
create more high excess nodes. This $\es$ subroutine is the only part of the
function that changes the $\sfrac{\abs{b_i^\mu}}{\Delta}$ ratio. 
\begin{claim}
The ratio $\sfrac{\abs{b_i^\mu}}{\Delta}$ is non-decreasing.
\end{claim}
\begin{proof}
$\alpha > 1$ by construction. The ratio for nodes in set $R$ is scaled by $\frac{\alpha}{\alpha}=1$ since
both the $\mu$ and $\Delta$ values are scaled. The ratio for nodes in 
the set $V\setminus R$ is multiplied by a factor of $\alpha > 1$ because only 
$\Delta$ is scaled (i.e. divided by $\alpha$). 
\end{proof}

Since this ratio is non-decreasing, the algorithm is always making progress towards finding a
contractible edge; however, this progress might be very slow. \frank{Is there
anything to say that it's not 0? Otherwise couldn't it just be stuck forever?}
To speed the process up, the algorithm prioritizes increasing the
$\sfrac{\abs{b_i^\mu}}{\Delta}$ ratio for nodes that are already approaching the
$20mn$ threshold; these nodes are included in set D.\frank{Same thing as before:
why this value?}

If there are no augmentations to be made on a certain iteration, $\es$ is called
if $D \cap V\setminus R \neq \varnothing$. Since $\es$ only increases
$\sfrac{\abs{b_i^\mu}}{\Delta}$ for nodes in $V\setminus R$, the subroutine will
only be called if that set contains nodes that are close to reaching $20mn$. If
there are no such nodes, a different subroutine, $\filtration$, is called first. \\ \\
\textbf{\textsc{Redistributing Excess}} $\filtration$ runs a modified max flow iteration on nodes in set $V \setminus R$;
the goal is to push flow directly to the sink from unreachable high excess
nodes (step 3c above). The flow values computed in the max flow replace the current flows on
edges contained in $V \setminus R$; all other edge flow values remain the same
except for flow on edges going from nodes in $V \setminus R$  to $R$ which are set
to zero\frank{again talking about cut might be clearer here}. This subroutine can speed up the finding the contractible arc process
in the following ways:

\begin{enumerate}[(i),itemsep=0mm]
\item Setting the flow to zero on edges from nodes in $V\setminus R$ to $R$
	reduces the excess of nodes in $R$, which could cause the nodes to become
	low-excess nodes that are reachable to augment to in future iterations
\item Reducing excess of nodes in $V \setminus R$ might cause some of these nodes
	to join set $D$, allowing their $\sfrac{\abs{b_i^\mu}}{\Delta}$ to be increased
	during $\es$
\end{enumerate}

This alternation between augmenting, rescaling, and pushing flow from
$V \setminus R$ to $t$ continues until the graph is contracted into one node. 

\subsection{Strongly Polynomial Bound}
In this section we analyze a few key ideas that lead to a strongly polynomial bounded runtime. The full (and lengthy) set of proofs for the runtime analysis can be found in the original paper \cite{article}.

First note that since the stopping condition for the ``find contractible edge'' is reducing the graph to one node, there can only be n-1 contractions. Therefore the analysis aims to put a strongly polynomial bound on the runtime of iterations between contractions (i.e. prove a contractible edge can be found within a polynomial number of steps). 

To bound the number of iterations between contractions, we define the following potential function: 

\[ \Psi \coloneqq \sum_{i \in H}  \lfloor \frac{e_i}{\Delta \mu_i} - (d_i + 1) \rfloor \]

The potential increases 

%This algorithm achieves a strongly polynomial runtime by improving upon scaling techniques used in previous weakly polynomial algorithms such as FAT-PATH \cite{Goldberg:1991:CAG:105014.105022} and also by integrating the concepts of abundant and contractible edges from Orlin's minimum cost flow algorithm \cite{Orlin1988}. 

%Orlin's \textit{abundant arc} concept can be applied to generalized max flow to show that if an edge has a large amount of flow in comparison to the value of the scaling factor $\Delta$, then that edge must have non-zero flow in the primal solution and, by complementary slackness, the edge must be tight in every dual solution. Therefore that edge can be contracted as shown in Section~\ref{sec:contract}. At a high-level, Végh's generalized max flow algorithm maintains a feasible primal solution while trying to identify these contractible edges to find an optimal dual solution. Once found, the optimal dual solution can be used to easily transform the feasible primal solution to an optimal primal solution via one traditional max flow iteration as shown in Section~\ref{sec:motivate-dual}.

%In order to find contractible edges in strongly polynomial time, this algorithm introduces the concept of \textit{continuous scaling}. Previous scaling algorithms for this problem use distinct $\Delta$-scaling phases, where $\Delta$ is only updated after a constant number of path augmentations. This can cause an ``over-shooting'' problem in which $\Delta$ increases too quickly relative to $f^\mu$ to guarantee abundant edges. As a solution, this algorithm continuously alternates between augmenting $\Delta$ units of flow from ``high-excess'' nodes to ``low-excess'' nodes (including the sink) and proportionately increasing $\mu$ and decreasing $\Delta$. Since $\Delta$ and $\mu$ are always scaled coherently, their relative magnitude stays bounded and non-decreasing, which addresses the over-shooting problem.
%\label{sec:2013}
	
	%This algorithm employs scaling parametrized by a factor $\Delta$.
    
   % Végh's initial strongly polynomial algorithm \cite{Vegh2013}
%	introduces the notion of a \textit{conservative labeling}, which helps to
%	identify the highest gain augmenting paths on which to push flow.
%	\begin{definition}
%	$\mu$ is a \textbf{conservative labeling} for $f$, a feasible solution to (P), if $\mu$ is
%	a feasible solution to (D) and all edges with positive flow are tight.
%	\end{definition}
 %   \begin{definition}
 %   $\mu$ is a $\Delta$-conservative labeling for $f$, or alternatively $(f, \mu)$ are a
%	$\Delta$-feasible pair, if
  %      \begin{enumerate}[(i)]
 %       \item $f_{ij}^\mu \leq \Delta$ for all non-tight edges $(i, j)$;
 %       \item $\mu_t = 1$; and
 %       \item $\mu_i > 0, \nfi - b_i \geq R_i$ for all $i \in V \setminus t$ where $R_i$ is the total incoming flow on non-tight edges
  %      \end{enumerate}
  % \end{definition}
    
%	The scaling algorithm aims to identify \emph{contractible} edges,
	%edges that are tight in every optimal dual solution,
  %  in a strongly polynomial number of steps. This technique was
  %  originally developed to study the minimum cost flow problem (\emph{cf.}~\cite{Orlin1988}). \todo{need to define this or refer to section 3?}
%	As we will show in Section~\ref{sec:infeasible} Lemma~\ref{lem.contractibility}, a large flow value along an edge can
 %   serve as a certificate of contractibility. To that end, we define
 %   \begin{definition}
 %   An edge $(i, j)$ is \textbf{abundant} at scale $\Delta$ if $f_{ij}^\mu \geq 17m\Delta$.
  %  \end{definition}
%		\todo{At least give intuition for how they derived this number}
%	Once the algorithm identifies an abundant edge,
%	it can contract the edge and reduce the size of the graph. The algorithm
%	terminates once the graph reduces to one node; this leads to a
%	strongly polynomial bound on the number of iterations of this algorithm.

%This algorithm also extends the scaling technique in the FAT-PATH algorithm
 %   \cite{Goldberg:1991:CAG:105014.105022} by introducing \textit{continuous scaling}. At a high-level, the continuous scaling
 %   method alternately (i) augments $\Delta$ units of flow along a tight path from high
 %   excess nodes to low excess nodes (including the sink) or (ii) proportionately increases $\mu$ and
  %  decreases $\Delta$. Unlike previous scaling methods used for the generalized flow problem, such as FAT-PATH \cite{Goldberg:1991:CAG:105014.105022},
 %   this algorithm does not have distinct $\Delta$-phases; instead, $\Delta$ is updated continuously.
    
  %  As $\Delta$ and $\mu$ are always scaled coherently, their relative magnitude will stay bounded.
 %   This addresses an ``over-shooting'' problem common in previous scaling algorithms in which $\Delta$
 %   increases too quickly relative to $f^\mu$ to guarantee abundant edges.
    
 %   As $\Delta$ changes, the algorithm maintains a $\Delta$-feasible pair, which ensures that there is
  %  a ``reachable'' feasible solution to (P). If $(f,\mu)$ is a $\Delta$-feasible pair,
  %  setting the flow on non-tight edges to zero yields a feasible pair.
 %   Given this, the algorithm only needs to run a cycle-canceling
 %   algorithm once on the initial solution and then feasibility is maintained throughout the rest of
 %   the algorithm.
    
	%\subsection{Algorithm Overview} \label{sec:2013-overview}
	%\begin{table}[H]
	%\begin{center}
	%    \begin{tabular}{ | p{7cm} | p{7cm} |}
	%    \hline
	%    Challenges in Previous Algorithms  & Solutions from Algorithm \\ \hline
	%    ``Abundant edge'' might not appear in strongly polynomial number of steps \cite{Radzik2004} & Continuous Scaling Method  \\ \hline
	%    Augmentation along highest gain paths introduce flow generating cycles; must run cycle-canceling algorithms in every scaling phase \cite{Goldberg:1991:CAG:105014.105022} & Maintain a $\Delta$-conservative pair, which guarantees feasible solution to (P); only need to run cycle-canceling once in initialization  \\ \hline
	%    ``Inflation'' in scaling algorithms; labels increase too much causing relabeled flow to be $\ll \Delta$. A new abundant edge may never appear. & Continuous scaling and Filtration subroutine \\
	%    \hline
	%    \end{tabular}
%	\end{center}
%	\caption{Key Improvements On Previous Algorithms}
%	\label{tab:improvementsInitial}
%	\end{table}
	
   % The key ideas that lead to improvements in this algorithm, as compared to previous algorithms,
   % are summarized in Table~\ref{tab:improvementsInitial}.
    
  % This algorithm is the first strongly polynomial algorithm for generalized maximum flow and has a running time of $O(n^3 m^2 \log n)$. In this section, we give an overview of the algorithm. In Section \ref{sec:tf}, \ref{sec:es}, and \ref{sec:filtration} we further explain the three main functions called in the algorithm. %This algorithm can be extended to  however, the $\log n$ term can be reduced by performing extra bookkeeping on flow values from previous iterations. This is explained in detail along with proofs of polynomially-bounded number sizes in Végh's full version of the paper~\cite{article}.
	     
	%Given an initial feasible solution to (P), the algorithm first runs a cycle-canceling algorithm,
	%	which gives a conservative labeling $\mu$ in strongly polynomial
	%	time\todo{?}. The bulk of the
  %  work in the algorithm is spent identifying and contracting abundant edges. This continues
  %  until the graph is contracted to one node. Once an optimal solution, $\mu$, to (D)
 %   is found for the contracted graph, it can easily be transformed into an optimal solution to
 %   (D) for the original graph. This is done by reversing the contraction step (full details
 %   are given in \cite{article}). The optimal dual solution can then be converted into an optimal solution to (P) by running one max flow computation (\tf\ subroutine). %\tf\ is then run one last time to convert the
    %optimal dual solution into an optimal solution to (P).
	
	%While finding and contracting edges, the algorithm maintains several sets as defined below:
	%\begin{align*}
	%N&\text{: set containing sink and ``low relabeled excess'' nodes with }
  %          \nfiu - \biu < (d_i +1)\Delta \\
	%T_0&\text{: set of ``high relabeled excess'' nodes with } \nfiu - \biu \geq (d_i + 2)\Delta \\
	%T&\text{: set of nodes reachable on a tight path from $T_0$ in $\Delta$-fat graph} \\ 
%	D&\text{: set of nodes $i \in V \setminus t$ with } \abs{b_i^\mu} \geq \Delta / n
	%\end{align*}
	
	%On each iteration, the algorithm augments along a tight path if possible. Otherwise, if there
  %  are no tight paths between nodes of high excess and nodes of low excess, the algorithm checks
   % if there are tight paths connecting high excess nodes to nodes not included in sets $T$ or $N$;
   % if so, a node along this tight path is added to set $T$. If the algorithm cannot push flow
  %  or expand the ``frontier'', then \filtration is called to reduce the excess of relatively
  %  isolated nodes. This is useful because it expands the set of ``low excess'' nodes that flow
  %  can be pushed to, which allows augmentation. It is important to note that this last option
  %  can only be computed at most $n - 1$ times because once a node is in set $D$, it cannot leave.
  %  This is because the ratio $\abs{b_i^\mu} / \Delta$ is non-decreasing. Below is a high-level
  %  overview of the procedure used to find contractible edges:
   % \todo{Use algorithm package if time}
%	\begin{align*}
%	&\text{IF $N \cap T \neq \varnothing$} \\
%	&\quad \text{Push $\Delta$ units of relabeled flow along a tight path from $i \in N$ to $j \in T$} \\ 
%	&\quad \text{Update sets accordingly} \\
%	&\text{ELSE ($N \cap T = \varnothing$)} \\
%	&\quad \text{IF $\exists$ tight path from $i \in T$ to $j \in V \setminus T$} \\
	%&\quad \quad \text{Add node $j$ to $T$} \\
	%&\quad \text{ELSE} \\
	%&\quad \quad \text{IF $(V \setminus T) \cap D = \varnothing$} \\
	%&\quad \quad \quad \text{\filtration($V \setminus T$)} \\
	%&\quad \quad \text{\es($T$)} 
	%\end{align*}
	
	%At the end of each iteration, the algorithm checks if an abundant edge has been
   % identified (and if so, it is contracted) or if the termination condition has been met.
  %  Once this loop terminates, the graph is expanded and a final max flow computation
  %  is run to find the optimal solution to (P).
%	\subsection{\tf\ Subroutine}\label{sec:tf}
    
%	The \tf\ subroutine is used at initialization to find an initial feasible flow $f$ and at
 %   termination to convert the optimal dual solution to an optimal solution to (P).
 %   It is also used in the \filtration\ subroutine (Section~\ref{sec:filtration}).
 %   This algorithm is essentially a standard maximum flow computation on the input
 %   set and an added source node. If the max flow computation is infeasible (due to the
 %   negative upper bounds), then an error is returned. We present the main theorem
 %   given in \cite{Vegh2013} below and also transform their explanation of the
 %   subroutine into pseudocode for a more clear representation.
	%\begin{theorem}
	%If $\mu$ is an optimal solution to (D), then $\tf(V, \mu)$ returns an optimal solution to (P).
    %\todo{this theorem is verbatim from paper---another way to phrase it?}
	%\end{theorem}
	%\todo{proof?}

	%\begin{align*}
	%&\text{\textbf{Input:} Set $S \subseteq V$ with $t \in S$ and labeling $\mu$ that is
   %        dual-feasible everywhere in $S$---i.e., $\giij \leq 1$ for all $i, j \in S$} \\
%	&\text{\textbf{Output:} Flow $f'$ such that $(f', \mu)$ is a conservative labeling
  %         everywhere in $S$} \\
%	&\text{Add source node $s$ and edges $(s, i)$ for all $i \in S \setminus t$} \\
%	&\text{Set upper and lower bound capacities, $u$ and $l$, as follows: } \\
%	&\quad \text{For all tight edges $(i, j)$: $u_{ij} = \infty$ and $l_{ij} = 0$} \\
%	&\quad \text{For all edges $(s, i)$: $u_{si} = -\biu$ and $l_{si} = -\infty$} \\
%	&\text{Run max flow $x$ on node set $S \cup \{s\}$ and the set of tight edges} \\
%	&\text{Return $f'$ such that}
 %   \end{align*}
 %   \[ f'_{ij} = \begin{cases} x_{ij} \mu_i & \text{if edge } (i, j) \text{ is tight} \\
 %   					       f'_{ij}      & \text{otherwise}
 %                \end{cases}
  %  \]
	
%	\subsection{Elementary Step Subroutine}\label{sec:es}
    
	%In a scaling step, $\Delta$ is continuously scaled down
  %%  and $\mu_i$ scaled up for $i \in T$. The flow on edges from $V \setminus T$ to $T$ and the flow
  %  on non-tight edges contained in $V \setminus T$ are also scaled down. This continues as long
  %  as $(f, \mu)$ remains $\Delta$-feasible and as long as the excess for each node $i$
 %   remains bounded by a quantity proportional to $\Delta$ and $\mu_i$.
 %   Once all values have been rescaled, the sets $T$ and $T_0$ are modified accordingly.  
	
%	\subsection{Filtration}\label{sec:filtration}
    
%	The \filtration\ subroutine is used to ``clean up'' the set of nodes $V \setminus T$ if
 %   all of the nodes have a high amount of excess relative to their demands. 
  %  It reduces the relabeled excess of these nodes,
  %  which increases the number of
  %  ``low excess'' nodes to push flow to,
  %  and then runs \tf\ on $V\setminus T$,
  %  which returns a new flow $f'$. The flow values are modified as follows: 
	%\begin{itemize}[itemsep=-1mm]
%	\item Edges entering $T$: $f_{ij}$ = 0
%	\item Edges leaving $T$: no change
%	\item Edges contained in $T$: no change
%	\item Edges contained in $V\setminus T$: $f_{ij} = f'_{ij}$
%	\end{itemize}
	
%	If these modifications reduce the excess of a node $i$ enough to either remove $i$ from set $T_0$
 %   ($i$ is no longer a high excess node) or add $i$ to set $N \cap T$ ($i$ is now a reachable
  %  low excess node), then the algorithm terminates and the next iteration of the
  %  ``finding a contractible edge'' is performed. Otherwise, the algorithm terminates
  %  and the elementary step routine is called to relabel.

	%	\todo{Need to tie everything together somehow}
	%	\todo{Runtime analysis?}

\section{A Faster Strongly Polynomial Algorithm}\label{sec:2017}

The most recent strongly polynomial algorithm developed by Olver and Végh \cite{Olver2017} takes the same overall approach as the original algorithm~\cite{Vegh2013}, but introduces several key ideas that improve the running time by a factor of almost $O(n^2)$. 
In this section, we describe the algorithm, tying core ideas back to previous work, and highlighting and providing intuition for the novel ideas that contribute to the improved running time. 
 %A highlight of the difficulties in
%previous algorithms and the subsequent solutions from this algorithm are
%summarized in Table~\ref{tab:improvements}.

%\begin{table}[H]
%\begin{center}
%\begin{tabular}{ | p{7cm} | p{7cm} | p{2cm} | }
  %  \hline
%		Challenges in Previous Algorithms  & Solutions from New Algorithm & Why \\ \hline
 %   Non-integral flow values & Only store relabeled flow $f^{\mu}$ which stays
%		integral throughout the algorithm until the last step of computing optimum & \\ \hline
 %   Initial cycle-canceling algorithm to eliminate flow generating cycles & Two
%		phase method (similar to simplex) where feasible solution found in first
%		phase is used to start the second phase; this solution is faster than the
	%	cycle-canceling subroutine & \\ \hline
 %   Complex methods for maintaining feasible flow & Relax flow feasibility (node
%		demands do not have to be met) and use complementary slackness properties to
%		keep feasible primal solution "within reach" & \\ \hline
%    No guarantee of abundant edge appearing in strongly polynomial number of
\iffalse		steps \cite{Radzik2004} &  Continuous scaling & \\ \hline
    Complicated multiplicative potential analysis \cite{Vegh2013} & Additive
		potential analysis & \\
    \hline
    \end{tabular}
\end{center}
\caption{Key Improvements of New Algorithm}
\label{tab:improvements}
\end{table}
\fi
	
	%%% TEMP %%% 
%%%	They want the set of tight edges, only at any given time actually have a dual solution. Only operations that happen to the edges are contractions or adding more tight edges, so tight edges never get removed. Inductive. The way to ensure tight edges never have to be removed is the safety property: ensure that their set of tight edges is always satisfiable, some flow that will only be supported on that set. Safety property can be maintained with the label update. 
	
%%%	Safety property is existence of a flow. Un-safety is equivalent to existence of Hoffman's property / cut. Scaling constructed so that it never breaks any of these sets. If there's a set that's violated in $\mu'$, then if you take the set excluding the parts that were modified then it must have already been violated. Modification only increased the values of $\biu$ inside $S$.
	
%%%	If there's a violated set, divided into two pieces, part in $S$, part not in $S$. One of them had to already be violated. Assume $\mu$ is safe and for the sake of contradiction that $\mu'$ is not safe. Then prove that if that were the case $\mu$ must have not been safe. 
%%%	$S$ is maximal subset that contains $Q$ and has no tight edges coming into it or out of it. Any modification done to $S$ is not going to change tight cuts because they must be either entirely inside or outside, not across the boundary, so scaled equally. 
	
%%%	Start with a feasible fitting flow. Find something close in $L_1$-norm (sum), decompose into paths to bound the difference between them. The difference between them is a flow and can't have any cycles because.
%%%	Important that $\mu$ be safe, because it assumes that there is a fitting pair and feasible $\mu$
%%%	If value of pseudoflow on edge is really high then optimal flow must have some flow on that edge becasue optimal flow has to be close enough to the pseudoflow
	
	%%% TEMP %%%
    
    %\label{sec:2017-intro}
    As in the initial strongly polynomial generalized flow algorithm, this algorithm makes progress by
    identifying contractible edges and then contracting them until a dual optimal
    solution is found. Each contraction collapses one node, so only $n$ contractions
    can occur. To identify and reason about contractible edges, we need a certificate
    (sufficient condition) for contractibility. This is an elaboration of the \emph{abundant edge}
    idea from Section \ref{sec:2013-notation}.

	\subsection{Working with Infeasible Flows}\label{sec:infeasible}
    
	Instead of maintaining a conservative pair of feasible flow $f$ and feasible
	dual solution $\mu$, the newer algorithm uses a possibly infeasible flow
	as a certificate for identifying contractible edges.
	More precisely,
    at each iteration of the algorithm we ensure that we have a \textbf{fitting pair}
	of primal and dual solutions $(f,\mu)$:
	\begin{definition}
	A (not necessarily feasible) flow $f$ is said to \textbf{fit} a feasible dual solution $\mu$
    if it satisfies the putative complementary slackness condition: $f_e > 0 \implies \giij = 1$.
    In this case, we refer to $(f, \mu)$ as a \textbf{fitting pair}.
	\end{definition}
    Note: as $f$ is \emph{not} required to be a feasible primal solution,
    this definition is a relaxation of the idea of
	conservative pair from Section \ref{sec:cert-contract}.
    
	In lieu of maintaining a feasible $f$ at each step,
	the algorithm ensures that $\mu$ is \textbf{safe}---i.e.,
    that a feasible $f$ fitting $\mu$ \emph{exists}---by preserving a
	``safety certificate''. To simplify the exposition, we define 
	\begin{definition}
		Recall that $\tau(\mu)$ denotes the set of tight edges under a dual labeling $\mu$.
		Given $\mu$, an \textbf{inward-loose cut} is a set $S \subseteq V$ that does
		not have any tight incoming edges 
		($\din(S) \cap \tau(\mu) = \varnothing$). An \textbf{outward-loose cut}
		does not have any tight outgoing edges ($\dout(S) \cap \tau(\mu) = \varnothing$). A \textbf{bi-loose cut}
		is both inward- and outward-loose.
	\end{definition}
	If $f$ is a flow fitting $\mu$, then there is no flow into inward-loose cuts.
	If $f$ is feasible, then it follows that the total demand inside an inward-loose
	cut is negative. \frank{why are these two statements true?}
	It turns out that this is a sufficient condition for the existence
	of a feasible fitting flow:
	\frank{I think we need to provide intuition for why this is the definition}
	\begin{lemma}[Certificate for Safety] \label{lem.safety}
	$\mu$ is safe if and only if for any inward-loose cut $S$,
	\[ \sum_{i \in S} \biu \leq 0. \]
	\end{lemma}
	This is Lemma 2.3 in \cite{Olver2017}, where it is described as a corollary of Hoffman's Circulation
	Theorem, Theorem 11.2 in \cite{Schrijver2002}. We provide a proof for completeness:
	\begin{proof}
		Suppose the cut condition holds. Let $f$ be a function on tight edges minimizing
		the total deficit $\Def(f, \mu) = \sum_i \min\{0, \biu - \nfiu\}$. Let
		$T^+$, $T^-$ be the sets of nodes with excess and deficit, respectively:
		\begin{align*}
			T^+ &= \{i \in V \mid \nfiu > \biu \} \\
			T^- &= \{i \in V \mid \nfiu < \biu \}.
		\end{align*}
		Consider the residual graph of $f$ in the tight graph. If there were a path
		connecting $T^+$ to $T^-$, then we could augment along the path
		to reduce the total deficit. Therefore, there is no such path. Consider the
		set $T$ of nodes from which $T^-$ is reachable along tight edges (including reverse edges).
		The absence of a residual path from $T^+$ to $T^-$
        implies that $T \cap T^+ = \varnothing$, whence
		\[ \sum_{i \in T} \nfiu - \biu = \sum_{i \in T^-} \nfiu - \biu < 0. \]
		By definition, $T$ is inward-loose. Therefore, the total flow into $T$ is zero.
        Moreover, the total flow out of $T$ is zero, for otherwise there would be
        residual tight edges into $T$. So
		\[ 0 = \sum_{e \in \din(T)} f_e - \sum_{e \in \dout(T)} f_e
		     = \sum_{i \in T} \nfiu < \sum_{i \in T} \biu, \]
		contradicting our assumption. It follows that $T^- = \varnothing$, i.e., $f$
		is feasible.
	\end{proof}
    
    \subsection{A Refined Contractibility Certificate}
    So long as the algorithm maintains the safety of $\mu$,
    we can obtain a feasible fitting flow $f$ at any time
    by running an ordinary maximum flow algorithm restricted to the tight graph of $\mu$.
    Moreover, a modified version of Lemma $\ref{lem.bound-dist}$ applies to fitting
    pairs.
    \begin{lemma} \label{lem.bound-dist2}
    Suppose $(g, \mu)$ is a fitting pair with $\mu$ safe. Then there is some optimal flow $g^*$
    uniformly ``close'' to $g$:
    \[ \|(g^*)^\mu - g^\mu\|_\infty \leq \Ex(g, \mu) + \Def(g, \mu). \]
    \end{lemma}
    \begin{proof}
    We show that there is a feasible $\tilde{g}$ fitting $\mu$ and satisfying
    \[ \|\tilde{g}^\mu - g^\mu\|_\infty \leq \Def(g, \mu) \quad \text{and} \quad
       \Ex(\tilde{g}, \mu) \leq \Ex(g, \mu). \]
    The conclusion then follows by Lemma \ref{lem.bound-dist} and the triangle inequality.
    
	As $g$ fits $\mu$, $g$ is supported on the tight edges $\tau(\mu)$. As
	$\mu$ is safe, there is some feasible flow supported in $\tau(\mu)$. We first
    look for a feasible flow $\tilde{g}$ close to $g$. To that end, consider the
    difference $f^\mu = \tilde{g}^\mu - g^\mu$, which is also supported in $\tau(\mu)$.
    (If $f_{ij}$ are negative, we think of this as positive flow on the reverse edge $(j, i)$.)
    Now minimize $\|f^\mu\|_1$, the total volume of difference,
    among all feasible fitting flows $\tilde{g}$. Adding cycles only increases $\|f^\mu\|_1$.
    Because $f$ is minimal with respect to this norm, it is acyclic. Thus it is a sum of
    paths whose total flow value can be measured at their endpoints:
    \[ \|f^\mu\|_\infty \leq \sum_i \max\{0, \nfiu\} = \sum_i \max\{0, -\nfiu\} = \frac{1}{2}\sum_i |\nfiu|
     = \frac{1}{2}\sum_i |\nabla \tilde{g}^\mu_i - \nabla g^\mu_i|. \]
    Now if $\nabla g_i^\mu < b_i^\mu$, then
    \[ \nfiu = \nabla \tilde{g}_i^\mu - \nabla g_i^\mu
     = (\nabla \tilde{g}_i^\mu - b_i^\mu) - (\nabla g_i^\mu - b_i^\mu) \geq b_i^\mu - \nabla g_i^\mu, \]
    as $\nabla \tilde{g}_i^\mu \geq b_i^\mu$ everywhere. So the total value of $f$ is at least
    $\Def(g, \mu)$. On the other hand, suppose it is more than $\Def(g, \mu)$. Then there
    is some node $i$ where $\nfiu > \max\{0, \biu - \nabla g_i^\mu\}$. So there is some path
    ending at $i$ whose value can be reduced to reduce $\|f\|_1$, while $\tilde{g}$
    remains feasible. Therefore, $\|f^\mu\|_\infty \leq \Def(g, \mu)$. Because $\tilde{g}$ is feasible,
    the paths shipping flow to fill the deficits of $g$ must begin at nodes at which $g$ has
    excesses. Therefore, $\Ex(\tilde{g}, \mu) \leq \Ex(g, \mu)$.
    \end{proof}
    
    Lemma \ref{lem.bound-dist2} suggests a more general certificate for contractibility:
    \begin{lemma} \label{lem.contractibility}
    Suppose $(g, \mu)$ is a fitting pair (with $g$ not necessarily feasible),
    $\mu$ is safe, and $g^\mu_e > \Ex(g, \mu) + \Def(g, \mu)$.
    Then $e$ is contractible.
    \end{lemma}

%	The safety of $\mu$ is important because it keeps the distance between
%	the (not necessarily feasible) primal $g$ we keep track of and the optimal
%	$g^{*}$ bounded. For example, consider the network in
%	Figure~\ref{fig:unsafe}, where $M$ is some arbitrarily large value. 
%	$g = 0$ fits $\mu = 1$, but $\mu$ is not safe because $b_s^{\mu} = 1 \nleq
%	0$. $\text{Ex}(g,\mu) + \text{Def}(g,\mu) = 1$, but the optimal solution is
%	clearly $g^{*}_{st} = 0, g^{*}_{ts} = M$, so
%	$||(g^{*})^{\mu} - g^{\mu}||_{\infty} = M$, which is unbounded.
%
%    \begin{figure}[h]
%    \centering
%    \includegraphics[width=0.45\textwidth]{figs/unsafe.pdf}
%    \caption{
%    \label{fig:unsafe}
%		If $\mu$ is not \textit{safe}, the distance between a $g$ fitting
%		$\mu$ and the optimal solution could be unbounded.
%    }
%    \end{figure}

	\subsection{Plentiful Nodes and Contraction}
    Lemma \ref{lem.contractibility} shows that a fitting pair with a very
    large flow value on one edge serves as a certificate that that edge is contractible.
    To develop a strongly polynomial algorithm, it remains to show that we can compute
    such a fitting pair.
    
    As stated, both conditions (bounded relabeled excess/deficit and
    large flow value on one edge)
    depend on the primal \emph{and} dual variables. In fact, we can associate each goal
    with one of the variables so that the algorithm may more easily work toward both goals:
    
    \begin{definition}
    A node $i$ is \textbf{plentiful} with respect
    to the current primal-dual solution pair $(f,u)$ if its scaled absolute
    demand is large relative to the size of the graph: $|b_i^{\mu}| \ge 3n(d_i + 1)$.
    \end{definition}
	\begin{theorem}
		Let $(g, \mu)$ be a fitting pair such that
		\begin{enumerate}[(i)]
			\item $\Ex(g, \mu) \leq 2n$ and $\Def(g, \mu) < n - 1$.
			\item $i$ is a plentiful node.
		\end{enumerate}
	\end{theorem}
	\begin{proof}
		Suppose that $i \in \vsrc$. The other case is similar. We have
		\[ \sum_{e \in \din(i)} g^\mu_e \geq
		\sum_{e \in \din(i)} g^\mu_e - \sum_{e \in \dout(i)} g^\mu_e
		= -\nabla g^\mu_i \geq -b_i^\mu - \Ex(g, \mu), \]
		where the last inequality is because $\nabla g_i^\mu - \biu$ appears
		as a term in $\Ex(g, \mu)$ if it is positive. Now
		\[ -b_i^\mu - \Ex(g, \mu) > 3n(d_i + 1) - 2n > 3nd_i > 3n|\dout(i)| \]
		So there is some edge $e \in \dout(i)$ with
		$g^\mu_e > 3n > \Ex(g, \mu) + \Def(g, \mu)$. This is an abundant and contractible
		edge.
	\end{proof}

    \subsection{Scaling and Augmentation}
    \label{sec:sub-ppn}
		The algorithm starts with a feasible fitting pair and consists of a sequence
    of epochs, each dedicated to identifying a contractible edge via a plentiful node.
    Each epoch alternates between the following augmentation and scaling phases:
    \begin{itemize}
    \item \textbf{Augmentation:} Reduce the total excess and deficit of the fitting flow $f$
    by augmenting as much flow as possible, one unit at a time, along tight paths from nodes with
		excess to nodes with deficit. Again we focus
		on tight paths because these are the highest-gain paths. Formally, let
    $D$ (``destinations'') be the union of the sink $t$ and all nodes with 
		deficit ($\nabla f_i^{\mu} < b_i^{\mu}$) and let $S$ (``sources'') be the
		set of all nodes $i$ for which $D$ is reachable from $i$ along a tight path
		in the residual graph. Continue augmenting along tight paths from $i$ to
		a node in  $D$ as long as there is some $i \in S$ for which the following conditions hold:
		%Let $S$ (``sources'') be
    %the set of all nodes $i$ for which
    \begin{enumerate}[(i)]
    \item $i$ has at least one unit of excess ($\nabla f_i^{\mu} \ge \lceil b_i^{\mu} \rceil$); and
    \item $b_i < 0$ (this condition is important in the potential analysis).
    \end{enumerate}
    %Augment until $S$ is empty.

    \item \textbf{Scaling:} Once it is not possible to augment any more flow, the second step scales down
    the labels and flow for all nodes and edges in $S$ by the same factor $\alpha$, 
		\frank{we do we only scale nodes in $S$? maybe have to be able to send
		excess somewhere to be conservative and only scale things where you can
		definitely send it }
    which is chosen to be the largest value that \rewrite{does not violate} any of the dual
    constraints~(D) or produces a plentiful node.
    \end{itemize}

    \todo{RE-WRITE AS DJIKSTRAS}

    Although it is not necessary in practice, for clarify of explanation, one
		can think of choosing the scaling factor $\alpha$ as solving     the following LP:
    \lpeq{\lpthree{max}
    {\alpha}
    {\gamma_{ij}^{\mu} \le 1\ \forall\ i,j \in \din(S)}
    {\nfiu - \biu \le 1}
    {\nexists\ \text{plentiful}\ i \in \vsink}
    }\frank{Should
			the last constraint be taken out? Because otherwise generating a plentiful
		node would not be a feasible solution...}



%    \begin{figure}[b!]
%    \centering
%    \includegraphics[width=0.35\textwidth]{figs/water.pdf}
%    \caption{
%    \label{fig:alpha}
%    Informal analogy for the maximization of scaling parameter $\alpha$.
%    Each pipe represents a constraint. $\alpha$ can only be increased until
%    one of them becomes tight as increasing it further would leak into
%    the lowest pipe and violate that constraint.
%    At each step, the order of which constraint becomes tight first may be different.
%    }
%    \end{figure}
%    Intuitively, as illustrated in Figure~\ref{fig:alpha}, this is akin to filling
%    a basin with water as much as possible without any of the water leaking into one
%    of three pipes, where each pipe represents a constraint and its height in the basin
%    represents the value of $\alpha$ at which point it becomes tight.
%
%    Thus, $\alpha$ is chosen such that one of the following constraints becomes
%    tight, or, in other words, one of the following becomes true corresponding to
%    each constraint, respectively:
%    \begin{enumerate}[(1),itemsep=-1mm]
%        \item An edge entering $S$ becomes tight, and thus a new node is added to $S$
%        \item A node in $S$ has excess $>1$, which allows us to augment flow from it
%        \item A node becomes plentiful
%    \end{enumerate}
%
%    If (3) is true, we're done. If (1) or (2) are true, it ensures that we will now
%    be able to make progress in the next iteration of the first step. Below, we show
%    in Lemma~\ref{lem:scaling} that the augmentations in the first step similarly ensure
%    that the algorithm can make progress in the second step. 

    \begin{figure}[b!]
    \centering
    \includegraphics[width=\textwidth]{figs/basin.pdf}
    \caption{
    \label{fig:alpha}
    Informal analogy for the maximization of scaling parameter $\alpha$.
    }
    \end{figure}
		Intuitively, as illustrated in Figure~\ref{fig:alpha}, this is akin to
		filling a basin with water, where the height of the water represents
		$\alpha$, and the successively

    Although it is conceptually easier to imagine increasing $\alpha$ continuously
    until one of these constraints becomes tight, in practice, it is possible to
    calculate $\alpha$ arithmetically in $O(n)$ by picking a value for each node
    that would satisfy one of the tightness conditions and then choosing the minimum
    such value so that only one of the constraints become tight and none are
    violated.
    The details of calculating the value are not particularly insightful so we omit them here, but the
    fact that it can be done is important for the running time analysis.

    We now make some important claims about the variables as the algorithm
    progresses that allow us to prove its correctness: i.e. that it terminates,
    producing a plentiful node.
    In the following section,
    we further prove that it terminates in strongly polynomial time and derive
    an upper bound on the running time.

    \begin{lemma}
    $f^{\mu}$ remains unchanged throughout the subroutine.
    \label{lem:fsame}
    \end{lemma} 
    \begin{proof}
        We must show that, for all edges $(i,j)$, $f_{ij}^{\mu}$ does not change.
        Recall that $f_{ij}^{\mu} = \frac{f_{ij}}{\mu_i}$. There
        are four possible cases for $(i, j)$:
        \begin{enumerate}
            \item Both outside of $S$. The subroutine only scales $i \in S$, so $\mu_i$ 
                does not change, and thus nor does $\fiju$.
            \item Both inside of $S$. The subroutine scales both $f_{ij}$ and $\mu$ by
                $\alpha$, which cancel each other out in the definition of $\fiju$.
            \item Only $i$ in $S$, which implies $i,j \in \dout(S)$. Then $f_{ij}=0$,
                because otherwise there would be a tight residual edge $j,i \in \din(S)$,
                and by definition $S$ has no incoming tight edges.
            \item Only $j$ in $S$, which implies $i,j \in \din(S)$. Again, incoming edges
                are not tight by definition of $S$ and we only augment along tight edges,
                so $f_{ij}$ is not augmented. Also, if $i \notin S$, then $\mu_i$ doesn't
                change. \qedhere
        \end{enumerate}
    \end{proof}
    \begin{corollary}
    $f^{\mu}$ is always integral.
    \end{corollary}
    \begin{proof}
    The augmentation step clearly maintains integrality of $f^{\mu}$ because we
    always augment by 1 unit at a time. The only other part of the algorithm that
    modifies the primal solution is the relabeling of $f_e$, and by
    Lemma~\ref{lem:fsame}, this never changes the value of $f^{\mu}$.
    \end{proof}
    \begin{lemma}
        \label{lem:scaling}
        For all nodes with non-zero demand in $S$, scaling $\mu$ in the second step
        strictly increases $|\biu|$. For all other nodes, $|\biu|$ remains unchanged.
        \todo{Show S cannot be empty?}
    \end{lemma}
    \begin{proof}
        First, trivially, scaling a node with zero demand cannot change its value
        ($0\cdot\mu=0$), and the algorithm only updates $\mu_i$ for $i \in S$. Recall
        that $\biu$ is defined as $b_i / \mu_i$, and thus, if $\mu_i\leftarrow \mu_i / \alpha$,
        then
        \[ \biu \leftarrow \frac{b_i}{\mu_i / \alpha} = \alpha\frac{b_i}{\mu_i} = \alpha \biu. \] 
        Now it suffices to prove that $\alpha > 1$ always holds when any of the three
        constraints become tight.

        By construction, before rescaling, $S$ has no tight
        incoming edges. Recall $\giij = \gamma_i\frac{u_i}{u_j}$. In order for an
        incoming edge $i \notin S, j\in S$ to become tight (1), we must have $\giij=1$:
        since we only scale values in $S$, $\alpha$ must be ${1}/{\giij}$, which is
        always greater than $1$ because $\giij < 1$ (by the feasibility of the dual).

        From the terminating condition of the augmentation step (the lack of any nodes
        in $S$ having excess), we know $\Ex(f) = \nfiu - \biu < 1$. Since, by
        Lemma~\ref{lem:fsame}, $\nfiu$ is unchanged and only nodes in $\vsrc$ (i.e. $b_i<0$)
        can have excess, the only way for to make $\Ex(f) \ge 1$ is if $\biu$ becomes
        more negative, which only happens if $\alpha > 1$. 

        Finally, we start out with the assumption that there are no plentiful nodes.
        In order for a node to become plentiful (3), its demand $\biu$ must increase,
        which is only possible if $\alpha > 1$.
    \end{proof}
    \begin{lemma}
        \label{lem:still-fit}
        $\fp$ remains a fitting pair throughout the subroutine.
    \end{lemma}
    \begin{proof}
        In order to be a fitting pair, we must have the following relationship between
        $f$ and $\mu$: for all $(i, j) \in E$, $\giij \leq 1$, and when $f_{ij} > 0$, 
        $\giij = 1$. \todo{This is a definition, not a proof.}
    \end{proof}

    \begin{lemma}
        The rescaling of $\mu$ during produce-plentiful-node does not break 
        the safety of $\mu$, i.e. there is a corresponding feasible primal
        solution even though we haven't kept track of it.
        \todo{Why is this here:} (~\ref{lem:usafe})
    \end{lemma}
    \begin{proof}
    The initialization subroutine is guaranteed to produce a fitting pair $\fp$,
    where $f$ is feasible, so we have that initially $\mu$ is safe. Now assume $\mu$
    is safe before running the subroutine. For sake of contradiction, assume that
    after running the subroutine, the updated labeling $\mu'$,
    is no longer safe. That is, there exists some subset of nodes $X \subseteq \vnott$  
    without any incoming tight edges ($\{\gamma_e^{\mu'} = 1\ \forall\ e \in \din(X)\} = \varnothing$)
    and where all nodes have positive demand $b^{\mu'}(X) > 0$.

    Suppose we divide $X$ into two parts: the subset of nodes in $S$, $X \cap S$, and those
    not in $S$, $X \setminus S$. Recall that $\mu_i' = \frac{\mu}{\alpha}$ for $i\in S$
    and $\mu' = \mu$ for $i \notin S$. Then we can rewrite the node demands under $\mu'$
    in terms of the demands under $\mu$ as follows:
    $$b^{\mu'}(X) = \frac{1}{\alpha}b^{\mu}(X \cap S) + b^{\mu}(X \setminus S)$$
    If, as we assumed, $b^{\mu'}(X) > 0$, then either $b^{\mu}(X \cap S)$ or 
    $b^{\mu}(X \setminus S)$ must be positive.
    \todo{Fill in details. $\mu'$ is defined in terms of $S$.}
    Now consider the set of edges coming into each of these subsets...
    Any edge that's tight under $\mu'$ was tight under $\mu$, because $\geu$, if
    it has no tight edges under $\mu'$ then it also had no tight edges under
    $\mu$.

    Therefore, since neither of the two parts of $X$
    have any tight incoming edges under $\mu$ and at least one of them
    must have strictly positive node demands, at least one of them
    violates the safety conditions of $\mu$ and thus $\mu$ was not safe.
    This is a contradiction, and thus if $\mu$ was safe, $\mu'$ must also be safe.

    \end{proof}

    In summary, we have shown that the primal and dual steps always allow room for the other to make progress, and collectively each iteration of the two steps strictly increases $|\biu|$ towards
    the definition of a plentiful node. Thus, the subroutine will eventually produce one. We have also shown that the updates it makes to $f$ and $\mu$ do not violate any of the previous conditions stated for being able to ultimately derive a primal optimal solution once our fitting pair finds the dual optimal. 

\iffalse
	 \subsection{Finding a feasible flow}
	\todo{Don't they just run the whole algorithm on a modified graph to initialize?}
	Drawing inspiration from the Simplex algorithm, the first step in this algorithm is finding a trivial feasible solution $(\bar{f}, \bar{\mu})$, which clearly also satisfies the definition of a fitting pair necessary for the algorithm. In order to find this initial solution, rather than using Radzik's cycle-cancelling subroutine as in prior work, the authors develop a new method based on negative cycle detection, which can be run in $O(nm)$ time. \todo{references} 
    \fi
    
    \subsection{Scaling and Rounding} 
    Since the second phase of the algorithm relies on the relabeled flow, $f^\mu$, being integral, we must first round the initial feasible flow (which does not have an integrality constraint) and then maintain its integrality throughout the second phase until finding the final optimum. Using the integrality properties of maximum flow, we can run a feasible circulation problem on the relabeled graph to find an integral max flow. We do this by taking the initial fitting pair $(f, \mu)$ and corresponding feasible relabeled flow $f^\mu$ and setting a lower bound of $\lfloor \nabla f_i^\mu \rfloor$ and an upper bound of $\lceil \nabla f_i^\mu \rceil$. The corresponding integral relabeled max flow on each edge $(i, j)$ can be turned into an integral flow by multiplying by $\mu_i$. Throughout the second phase, the value of $f_{ij}^\mu$ remains unchanged because if $f_{ij}$ is scaled, the algorithm scales $\mu_i$ by the same amount; therefore, $f_{ij}^\mu$ remains integral after the initial rounding.
    
    In addition to rounding the initial feasible flow, the initial labels, $\mu$, are scaled at the beginning of phase 2. Before rounding, we set a scaling factor $\Delta$ equal to $\max_{i \in V \setminus t} \nfiu - \biu$, which is the maximum excess at a node in the initial feasible solution. We multiply $\mu$ by $\Delta$, which reduces $f^\mu$ by a factor of $\Delta$. This scaling is useful because it bounds $\nfiu$, which is used to bound the total excess in the amortized analysis (explained in Section 5.8). The resulting bound is $\biu - 1 \leq \nfiu \leq \biu + 2$ for all $i \in V \setminus t$; the derivation leading to this result is not explicitly explained in the original paper \cite{Olver2017}, but we provide a clearer explanation. Initially, $\nfiu \geq \biu$ because it must be feasible. After rounding, $\nfiu \geq \biu - 1$ because $\nfiu$ decreases by at most $1$ during rounding. This gives the lower bound of the result. The upper bound comes from the scaling and the definition of $\Ex(f, \mu)$ in Section~\ref{sec:lp}. Since we scale $\nfiu$ down by the maximum excess in the initial solution, the maximum in $\Ex(f,\mu)$ will never be more than $2$. This gives the upper bound $\nfiu \leq \biu + 2$. 

    \subsection{Running Time Analysis}
    The algorithm consists of three basic types of operations: scaling, augmentation, and contraction.
    The number of contractions is bounded by $n$. Contraction itself
    requires computing a regular flow and performing a bounded number of graph operations, but
    strongly polynomial algorithms already exist for these. So the goal is to bound the running time
    (number of arithmetic operations) between any two contractions.

    Augmentation itself consists of a graph search on the tight subgraph, an $O(m)$ operation.
    Between rounds of augmentation, there is a scaling round. Each atomic scaling step stops
    when either (i) a node has become plentiful, leading to a contraction; (ii) a node has an
    excess greater than $1$, leading to a new augmentation round; or (iii) the loose cut
    $S$ has been extended by at least one node. (iii) can only happen at most $n$ times; thus,
    there are at most $n$ scaling steps between any two augmentations. Each scaling step
    is an $O(n)$ operation, leading to a naive time bound of $O(n^2)$ between any two augmentations.
    This can be improved to $O(m + n \log n)$ by applying the framework of Dijkstra's
    algorithm with costs $-\log \gamma^u_e$. The loose cut $S$ corresponds to the expanding
    frontier of Dijkstra's algorithm.

    It remains to bound the total number of augmentations the algorithm may perform. We'd
    like to show that after a strongly polynomial number of augmentations, the algorithm
    is guaranteed to find a plentiful node.

    \begin{lemma} \label{lem.num-aug}
        After $O(mn)$ augmentations, the algorithm is guaranteed to find a plentiful node.
    \end{lemma}
    First, a preliminary lemma. We denote the period between successive contraction
    steps as an epoch.
    \begin{lemma}
    At any node $i \in \vsrc$, the deficit is at most one. Moreover, at most one augmentation
    ends at such a node during any epoch.
    \end{lemma}
    \begin{proof}
        At the beginning of the algorithm, the flow is feasible, so $\nfiu - \biu \geq 0$.
        Rounding decreases $\nfiu$ by less than one. Similarly, during each contraction,
        the adjusted flow is computed with $nabla g_i^\mu \geq \lfloor \biu \rfloor$ everywhere.

        Between contractions, the deficit can be modified by scaling and augmentation steps.
        Since $i \in \vsrc$, $\biu < 0$, and so $\biu$ can only decrease during scaling.
        Meanwhile, augmentations only decrease deficits.
        Once $\nfiu \geq \biu$, no augmentation ending at $i$ will be contemplated unless $i$
        experiences a deficit again. The only way for this to happen would be for
        augmentations beginning at $i$ to reduce $\nfiu$ below $\biu$. However,
        an augmentation will only begin at $i$ if
        $\nfiu - \biu \geq 1$, and will reduce $\nfiu$ by exactly one. Hence, $i$ will
        never again experience a deficit during the epoch, and so there will be no further
        augmentations ending at $i$.
    \end{proof}

    \begin{proof}[Proof of Lemma \ref{lem.num-aug}]
    Define two potential functions
    \begin{align*}
    \Psi(\mu) &= - \sum_{i \in \vsrc} \biu \\
    \Phi(\mu) &= \sum_{i \in \vsrc} \nfiu + \Psi(\mu) = \sum_{i \in \vsrc} \nfiu - \biu
               \leq \Ex(\mu)
    \end{align*}
    \todo{complete proof}
    \end{proof}


\section{Conclusion}\label{sec:discussion} 

We presented and compared two strongly polynomial algorithms for the maximum generalized flow problem. Both algorithms obtain optimal solutions by solving the relabeled dual problem and by identifying and contracting abundant edges. The more recent algorithm by Olver and Végh achieves a theoretical bound that is almost a factor of $O(n^2)$ faster than Végh's initial algorithm though by relaxing the feasibility of flow during the algorithm. \todo{brief explanation of key factor(s) that causes the new algorithm to have much better speed up}

%	\subsection{Non-triviality of a strongly polynomial algorithm}

%	\todo{Explain} why applying ideas from strongly polynomial algorithms for
%	min-cost flow was not straightforward. 

%	We know that the feasibility problems are the same for min-cost and
	%generalized max-flow. Min-cost objective is additive: when you change the
	%value of the flow on one edge, that doesn't affect the objective piece for any
	%other edge, whereas in the
    
    %feedback 
    \vspace{10mm}
\todo{Main feedback from emails:} 

\todo{(2) too many definitions}

\todo{(3) paper needs more synthesis and should have explicit outline and more main ideas rather than full proofs}

\todo{(4) make $\Delta$-scaling part more clear}

\todo{(5) Section 4 too definition heavy; needs more intuition}

	%feedback

\setlength{\bibitemsep}{0pt}
\nocite{*}
\printbibliography
\end{document}
 
